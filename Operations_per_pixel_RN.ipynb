{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9874f277",
   "metadata": {},
   "source": [
    "# Analyse du nombre d'opérations par pixel\n",
    "\n",
    "Ce notebook propose une analyse mathématique du nombre d'opérations (multiply‑add) par pixel  pour deux réseaux de débruitage populaires : **FFDNet** et **DRUNet**.     Nous utilisons les implémentations officielles (IPOL pour FFDNet, DeepInv pour DRUNet) afin de charger les modèles, puis nous comparons :\n",
    "\n",
    "- une **estimation analytique**, basée sur les paramètres exacts du modèle chargé (nombre de couches, canaux, tailles de noyaux et facteurs de sous‑échantillonnage/sur‑échantillonnage) ;\n",
    "- une **estimation numérique** en évaluant les FLOPs totaux via `ptflops` et en normalisant par le nombre de pixels de l'entrée.\n",
    "\n",
    "Afin d'éviter toute ambiguïté sur l'entrée du modèle, nous enveloppons les réseaux pour y fixer explicitement le paramètre de bruit (`noise_sigma` )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1463ba",
   "metadata": {},
   "source": [
    "## 1. Chargement de FFDNet et analyse\n",
    "\n",
    "Nous téléchargeons la version IPOL de FFDNet, importons dynamiquement `models.py`,     instancions le modèle couleur (3 canaux) et chargeons ses poids pré‑entraînés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb29424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement FFDNet (IPOL)…\n",
      "Extrait dans ffdnet_ipol/\n",
      "FFDNet (RGB) chargé et prêt\n"
     ]
    }
   ],
   "source": [
    "import os, sys, zipfile, urllib.request, torch\n",
    "\n",
    "URL_CODE = \"https://www.ipol.im/pub/art/2019/231/ffdnet-pytorch39.zip\"\n",
    "ZIP_PATH, EXTRACT_DIR = \"ffdnet-pytorch39.zip\", \"ffdnet_ipol\"\n",
    "\n",
    "# 1) Téléchargement + extraction \n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    if not os.path.exists(ZIP_PATH):\n",
    "        print(\"Téléchargement FFDNet (IPOL)…\")\n",
    "        urllib.request.urlretrieve(URL_CODE, ZIP_PATH)\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(EXTRACT_DIR)\n",
    "    print(f\"Extrait dans {EXTRACT_DIR}/\")\n",
    "\n",
    "# 2) Ajouter le dossier aux paths\n",
    "FFDNET_DIR = os.path.join(EXTRACT_DIR, \"ffdnet-pytorch\")\n",
    "sys.path.append(FFDNET_DIR)\n",
    "from models import FFDNet  \n",
    "\n",
    "# 3) Localiser les poids dans le dossier extrait\n",
    "candidates = [\n",
    "    os.path.join(FFDNET_DIR, \"models\", \"net_rgb.pth\"),\n",
    "    os.path.join(FFDNET_DIR, \"net_rgb.pth\"),\n",
    "]\n",
    "\n",
    "WEIGHTS_PATH = next((p for p in candidates if os.path.exists(p)), None)\n",
    "assert WEIGHTS_PATH, \"Poids 'net_rgb.pth' introuvables dans l'archive extraite.\"\n",
    "\n",
    "# 4) Charger le modèle + poids\n",
    "ffdnet = FFDNet(num_input_channels=3)\n",
    "state = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n",
    "ffdnet.load_state_dict(state.get(\"state_dict\", state), strict=False)\n",
    "ffdnet.eval()\n",
    "print(\"FFDNet (RGB) chargé et prêt\")\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ac02e",
   "metadata": {},
   "source": [
    "### 1.1 Estimation analytique pour FFDNet\n",
    "\n",
    "FFDNet commence par un **sous-échantillonnage ×2** en hauteur et en largeur, produisant 4 sous-images.  \n",
    "Toutes les convolutions 3×3 opèrent donc sur cette résolution réduite d’un facteur 4.  \n",
    "Pour revenir au coût par pixel de l’image d’origine, on divise systématiquement par 4.\n",
    "\n",
    "On note :\n",
    "- $C$ : nombre de canaux de l’image (3 pour la couleur, 1 pour le gris),\n",
    "- $s$ : nombre de cartes de bruit concaténées à l’entrée,\n",
    "- $W$ : largeur interne (nombre de feature maps de la première convolution),\n",
    "- $D$ : nombre total de convolutions dans le réseau.\n",
    "\n",
    "La formule analytique du coût par pixel est :\n",
    "\n",
    "$$\n",
    "\\mathrm{MACs/pixel} \\;=\\;\\tfrac{9}{4}\\Big( W(4C+s) \\;+\\; (D-2)W^2 \\;+\\; 4CW \\Big).\n",
    "$$\n",
    "\n",
    "- Le premier terme $W(4C+s)$ correspond à la **première convolution** (entrée → largeur interne).  \n",
    "- Le terme $(D-2)W^2$ regroupe les **convolutions intermédiaires** de largeur constante $W$.  \n",
    "- Le dernier terme $4CW$ correspond à la **dernière convolution** (largeur interne → sortie avec $4C$ canaux).  \n",
    "- Le facteur $9$ provient du noyau $3\\times3$, et la division par 4 compense la résolution réduite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93a8d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs/pixel : 213192.0\n",
      "Opération/pixel : 426384.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def ffdnet_macs_per_pixel(model, C=3, s=1):\n",
    "    \"\"\"\n",
    "    Calcul du coût analytique en MACs/pixel pour FFDNet\n",
    "\n",
    "        MACs/pixel = (9/4) [ W(4C+s) + (D-2)W^2 + 4CW ]\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Instance FFDNet.\n",
    "    C : int\n",
    "        Nombre de canaux de l'image (3 = couleur, 1 = gris).\n",
    "    s : int\n",
    "        Nombre de cartes sigma (1 par canal).\n",
    "    \"\"\"\n",
    "    # Nombre total de convolutions\n",
    "    convs = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]\n",
    "    D = len(convs)\n",
    "\n",
    "    # Largeur interne = nombre de cartes de la première conv (out_channels)\n",
    "    W = convs[0].out_channels\n",
    "\n",
    "    # Application de la formule analytique\n",
    "    macs_per_pixel = (9/4) * (W * (4*C + s) + (D - 2) * W * W + 4*C * W)\n",
    "    return macs_per_pixel\n",
    "\n",
    "\n",
    "analytic_ffdnet = ffdnet_macs_per_pixel(ffdnet, C=3, s=3)\n",
    "print(\"MACs/pixel :\", analytic_ffdnet)\n",
    "print(\"Opération/pixel :\", analytic_ffdnet * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29b34e",
   "metadata": {},
   "source": [
    "### 1.2 Estimation numérique via ptflops\n",
    "\n",
    "L'implémentation FFDNet IPOL s'utilise comme `out = model(x, noise_sigma)` où `noise_sigma` est un scalaire par image.     Afin d'éviter les erreurs lors de l'appel par `ptflops`, on encapsule le modèle dans un wrapper     qui injecte automatiquement `noise_sigma` et expose un `forward(x)` classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba115a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs/pixel (numérique) : 214200.0\n",
      "Opérations/pixel (numérique) : 428400.0\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFDNetWrapper(nn.Module):\n",
    "    def __init__(self, model, sigma=25.0/255.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sigma = sigma\n",
    "    def forward(self, x):\n",
    "        # FFDNet IPOL attend un tenseur 1D de taille (batch,) pour noise_sigma\n",
    "        noise_sigma = torch.tensor([self.sigma], device=x.device)\n",
    "        return self.model(x, noise_sigma)\n",
    "\n",
    "ffdnet_wrap = FFDNetWrapper(ffdnet, sigma=25.0/255.0)\n",
    "\n",
    "# Mesure des Opérations totaux avec ptflops\n",
    "macs_ffdnet, _ = get_model_complexity_info(\n",
    "    ffdnet_wrap, (3, 256, 256), as_strings=False, print_per_layer_stat=False\n",
    ")\n",
    "print(\"MACs/pixel (numérique) :\", macs_ffdnet / (256 * 256))\n",
    "print(\"Opérations/pixel (numérique) :\", macs_ffdnet * 2 / (256 * 256))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5a5ba",
   "metadata": {},
   "source": [
    "## 2. Chargement de DRUNet et analyse\n",
    "\n",
    "Nous utilisons la librairie `deepinv` pour télécharger et instancier DRUNet pré‑entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bd736ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRUNet chargé\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import deepinv as dinv\n",
    "\n",
    "drunet = dinv.models.DRUNet(pretrained=\"download\").eval()\n",
    "print(\"DRUNet chargé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce591d2",
   "metadata": {},
   "source": [
    "### 2.1 Estimation analytique pour DRUNet\n",
    "\n",
    "DRUNet suit une architecture de type **U-Net résiduel**, avec une tête de convolution, un encodeur multi-niveaux, un bottleneck, puis un décodeur symétrique.  \n",
    "On peut récupérer ses hyperparamètres directement depuis le modèle :\n",
    "- `nc` : liste du nombre de canaux à chaque niveau (ex. [64, 128, 256, 512]),\n",
    "- `nb` : nombre de blocs résiduels par niveau (chaque bloc = 2 convolutions 3×3),\n",
    "- `in_nc` : nombre de canaux d’entrée (en général 3),\n",
    "- un canal supplémentaire est ajouté pour la carte de bruit.\n",
    "\n",
    "Chaque convolution est comptée en multiplications-accumulations (MACs), puis normalisée par pixel d’entrée grâce aux facteurs de résolution :\n",
    "\n",
    "$$\n",
    "r_\\ell = \\frac{1}{4^\\ell}, \\quad \\ell=0,\\dots,L,\n",
    "$$\n",
    "\n",
    "avec $L=3$ pour un encodeur à 3 niveaux (résolutions relatives : 1, 1/4, 1/16, 1/64).\n",
    "\n",
    "La formule est donc :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{MACs/pixel} \\;=\\;& \\underbrace{9\\,r_0\\,(in_{c}+1)\\,nc_0}_{\\text{tête}} \\\\\n",
    "&+ \\sum_{i=0}^{L-1} \\Big[ \\underbrace{18\\,r_i\\,nb\\,nc_i^2}_{\\text{blocs enc}} + \\underbrace{4\\,r_{i+1}\\,nc_i\\,nc_{i+1}}_{\\text{down $2\\times2$}} \\Big] \\\\\n",
    "&+ \\underbrace{18\\,r_L\\,nb\\,nc_L^2}_{\\text{bottleneck}} \\\\\n",
    "&+ \\sum_{i=L-1}^{0} \\Big[ \\underbrace{4\\,r_i\\,nc_{i+1}\\,nc_i}_{\\text{up $2\\times2$}} + \\underbrace{18\\,r_i\\,nb\\,nc_i^2}_{\\text{blocs dec}} \\Big] \\\\\n",
    "&+ \\underbrace{9\\,r_0\\,nc_0\\,in_c}_{\\text{queue}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "- La **tête** projette $(in\\_nc+1)$ canaux (image + carte de bruit) vers $nc_0$.  \n",
    "- Chaque **bloc résiduel** contient deux convolutions 3×3 → facteur $18\\,r_i\\,nc_i^2$.  \n",
    "- Les **downsamplings** sont modélisés par des convolutions 2×2 stride 2.  \n",
    "- Les **upsamplings** par des transposed-conv 2×2 stride 2.  \n",
    "- La **queue** ramène $nc_0 \\to in\\_nc$.  \n",
    "- Aucun terme de “fusion” séparée n’est ajouté : la concaténation des skip connections est absorbée dans le premier bloc du décodeur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5080898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs/pixel (analytique, DRUNet) : 2191296.0\n",
      "FLOPs/pixel (analytique) : 4382592.0\n"
     ]
    }
   ],
   "source": [
    "def drunet_macs_per_pixel(model):\n",
    "    # Paramètres du modèle (avec valeurs par défaut)\n",
    "    nc = model.nc if hasattr(model, 'nc') else [64, 128, 256, 512]\n",
    "    nb = model.nb if hasattr(model, 'nb') else 4\n",
    "    in_nc = model.in_nc if hasattr(model, 'in_nc') else 3\n",
    "\n",
    "    L = len(nc) - 1  # nombre de transitions (niveaux enc/dec)\n",
    "    res_factors = [1.0 / (4 ** i) for i in range(L + 1)]  # r_i = 1/4^i\n",
    "\n",
    "    ops = 0.0\n",
    "\n",
    "    # Tête (3x3): (C + sigma) -> n0, comptée à r0\n",
    "    ops += (in_nc + 1) * nc[0] * 9 * res_factors[0]\n",
    "\n",
    "    # Encodeur: blocs + down\n",
    "    for i in range(L):\n",
    "        ch = nc[i]\n",
    "        # nb blocs résiduels (2 conv 3x3) à r_i\n",
    "        ops += nb * 2 * ch * ch * 9 * res_factors[i]\n",
    "        # down: conv 2x2 stride 2, coût à r_{i+1}\n",
    "        ops += ch * nc[i + 1] * 4 * res_factors[i + 1]\n",
    "\n",
    "    # Bottleneck au niveau L\n",
    "    ch_body = nc[-1]\n",
    "    ops += nb * 2 * ch_body * ch_body * 9 * res_factors[L]\n",
    "\n",
    "    # Décodeur: up + blocs (sans fusion séparée)\n",
    "    for i in reversed(range(L)):\n",
    "        ch_in = nc[i + 1]\n",
    "        ch_out = nc[i]\n",
    "        # up: deconv 2x2 stride 2, coût à r_i\n",
    "        ops += ch_in * ch_out * 4 * res_factors[i]\n",
    "        # blocs résiduels (2 conv 3x3) à r_i\n",
    "        ops += nb * 2 * ch_out * ch_out * 9 * res_factors[i]\n",
    "\n",
    "    # Queue (3x3): n0 -> C à r0\n",
    "    ops += nc[0] * in_nc * 9 * res_factors[0]\n",
    "\n",
    "    return ops\n",
    "\n",
    "analytic_drunet = drunet_macs_per_pixel(drunet)\n",
    "print(\"MACs/pixel (analytique, DRUNet) :\", analytic_drunet)\n",
    "print(\"FLOPs/pixel (analytique) :\", analytic_drunet * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fef6e",
   "metadata": {},
   "source": [
    "### 2.2 Estimation numérique via ptflops pour DRUNet\n",
    "\n",
    "L'implémentation DeepInv de DRUNet utilise la signature `forward(x, sigma)`.     Pour éviter l'erreur de paramètre manquant, on enveloppe également DRUNet dans un wrapper qui fixe `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afbb00de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs/pixel (numérique) : 2119424.0\n",
      "FLOPs/pixel (numérique) : 4238848.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch.nn as nn\n",
    "\n",
    "class DRUNetWrapper(nn.Module):\n",
    "    def __init__(self, model, sigma=25.0/255.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sigma = sigma\n",
    "    def forward(self, x):\n",
    "        # sigma est un scalaire passé à drunet\n",
    "        return self.model(x, sigma=self.sigma)\n",
    "\n",
    "# Envelopper le modèle\n",
    "wrapped_drunet = DRUNetWrapper(drunet, sigma=25.0/255.0)\n",
    "\n",
    "macs_drunet, _ = get_model_complexity_info(\n",
    "    wrapped_drunet, (3, 256, 256), as_strings=False, print_per_layer_stat=False\n",
    ")\n",
    "print(\"MACs/pixel (numérique) :\", macs_drunet / (256 * 256))\n",
    "print(\"FLOPs/pixel (numérique) :\", (macs_drunet * 2) / (256 * 256))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127922d",
   "metadata": {},
   "source": [
    "## 3. Synthèse des résultats\n",
    "\n",
    "Le fonction ci‑dessous compare les estimations analytiques et numériques pour les deux modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8624707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison MACs/pixel : FFDNet ===\n",
      "---------------------------------------\n",
      "Analytique (MACs/pixel) : 213192.00\n",
      "Numérique (MACs/pixel)  : 214200.00\n",
      "Écart absolu            : 1008.00\n",
      "Écart relatif (%)       : 0.473%\n",
      "\n",
      "=== Comparaison MACs/pixel : DRUNet ===\n",
      "---------------------------------------\n",
      "Analytique (MACs/pixel) : 2191296.00\n",
      "Numérique (MACs/pixel)  : 2119424.00\n",
      "Écart absolu            : 71872.00\n",
      "Écart relatif (%)       : 3.280%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compare_macs_model(model_name, macs_analytic, macs_numeric):\n",
    "    \"\"\"\n",
    "    Compare deux estimations de MACs/pixel (analytique vs numérique)\n",
    "    pour un modèle donné (FFDNet, DRUNet, ...).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Nom du modèle (ex: \"FFDNet\", \"DRUNet\").\n",
    "    macs_analytic : float\n",
    "        Valeur analytique (MACs/pixel).\n",
    "    macs_numeric : float\n",
    "        Valeur numérique (ptflops, MACs/pixel).\n",
    "    \"\"\"\n",
    "    diff_abs = np.abs(macs_numeric - macs_analytic)\n",
    "    diff_rel = 100 * diff_abs / macs_analytic if macs_analytic != 0 else float(\"nan\")\n",
    "\n",
    "    print(f\"=== Comparaison MACs/pixel : {model_name} ===\")\n",
    "    print(39 * \"-\")\n",
    "    print(f\"Analytique (MACs/pixel) : {macs_analytic:.2f}\")\n",
    "    print(f\"Numérique (MACs/pixel)  : {macs_numeric:.2f}\")\n",
    "    print(f\"Écart absolu            : {diff_abs:.2f}\")\n",
    "    print(f\"Écart relatif (%)       : {diff_rel:.3f}%\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "compare_macs_model(\"FFDNet\", analytic_ffdnet, macs_ffdnet / (256 * 256))\n",
    "compare_macs_model(\"DRUNet\", analytic_drunet, macs_drunet / (256 * 256))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
